import math
import numpy as np
from typing import Tuple

import altair as alt
import pandas as pd
import streamlit as st


# ===================== åŸæœ‰æ ¸å¿ƒå‡½æ•°ï¼ˆä¿ç•™11.pyå…¨éƒ¨é€»è¾‘ï¼‰ =====================
def normal_cdf(z: float) -> float:
    """Standard normal CDF using erf to avoid extra dependencies."""
    return 0.5 * (1 + math.erf(z / math.sqrt(2)))


def normal_ppf(p: float) -> float:
    """Approximate inverse CDF for standard normal (Acklam's approximation)."""
    if p <= 0.0:
        return -math.inf
    if p >= 1.0:
        return math.inf

    a = [
        -3.969683028665376e01,
        2.209460984245205e02,
        -2.759285104469687e02,
        1.383577518672690e02,
        -3.066479806614716e01,
        2.506628277459239e00,
    ]
    b = [
        -5.447609879822406e01,
        1.615858368580409e02,
        -1.556989798598866e02,
        6.680131188771972e01,
        -1.328068155288572e01,
    ]
    c = [
        -7.784894002430293e-03,
        -3.223964580411365e-01,
        -2.400758277161838e00,
        -2.549732539343734e00,
        4.374664141464968e00,
        2.938163982698783e00,
    ]
    d = [
        7.784695709041462e-03,
        3.224671290700398e-01,
        2.445134137142996e00,
        3.754408661907416e00,
    ]

    plow = 0.02425
    phigh = 1 - plow

    if p < plow:
        q = math.sqrt(-2 * math.log(p))
        return (
            (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])
            / ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1)
        )
    if p > phigh:
        q = math.sqrt(-2 * math.log(1 - p))
        return -(
            (((((c[0] * q + c[1]) * q + c[2]) * q + c[3]) * q + c[4]) * q + c[5])
            / ((((d[0] * q + d[1]) * q + d[2]) * q + d[3]) * q + 1)
        )

    q = p - 0.5
    r = q * q
    return (
        (((((a[0] * r + a[1]) * r + a[2]) * r + a[3]) * r + a[4]) * r + a[5]) * q
    ) / (
        (((((b[0] * r + b[1]) * r + b[2]) * r + b[3]) * r + b[4]) * r + 1)
    )


def normal_pdf(z: float) -> float:
    return (1 / math.sqrt(2 * math.pi)) * math.exp(-0.5 * z * z)


def truncated_normal_mean(mu: float, sigma: float, a: float) -> float:
    """Mean of truncated normal distribution, truncated at a (lower bound)."""
    if sigma <= 0:
        return max(mu, a)
    z_a = (a - mu) / sigma
    phi_a = normal_pdf(z_a)
    Phi_a = normal_cdf(z_a)
    if Phi_a >= 1:
        return mu
    return mu + sigma * (phi_a / (1 - Phi_a))


def truncated_normal_var(mu: float, sigma: float, a: float) -> float:
    """Variance of truncated normal distribution, truncated at a (lower bound)."""
    if sigma <= 0:
        return 0.0
    z_a = (a - mu) / sigma
    phi_a = normal_pdf(z_a)
    Phi_a = normal_cdf(z_a)
    if Phi_a >= 1:
        return sigma ** 2
    ratio = phi_a / (1 - Phi_a)
    var = sigma ** 2 * (1 - ratio * (z_a + ratio))
    return max(0.0, var)  # Ensure non-negative


def estimate_overall_from_data(
    applicants: float,
    interview_count: float,
    min_written: float,
    estimate_mode: str = "å·²çŸ¥æœ€é«˜åˆ†",
    known_max: float = None,
    historical_min: float = None,
    historical_max: float = None,
    ratio: float = None,
) -> Tuple[float, float]:
    """Estimate overall written mean and sd."""
    if applicants <= 0 or interview_count <= 0 or min_written <= 0:
        return 0.55 * 200, 0.15 * 200  # Fallback

    p = interview_count / applicants
    if p >= 1:
        return min_written, 1.0
    if p <= 0:
        return min_written, 1.0

    z = normal_ppf(1 - p)

    # Base (fallback) estimate
    sigma = min_written / (z + 1.5)
    mu = min_written - sigma * z

    # Adjust based on mode
    if estimate_mode == "å·²çŸ¥æœ€é«˜åˆ†" and known_max is not None and known_max > min_written:
        try:
            z_min = z  # corresponds to min_written quantile (1 - p)
            z_max = normal_ppf(1.0 - 1.0 / (applicants + 1.0))
            if abs(z_max - z_min) > 1e-6:
                sigma = (known_max - min_written) / (z_max - z_min)
                mu = min_written - z_min * sigma
            else:
                z_max = max(z_min + 2.5, 3.0)
                sigma = (known_max - min_written) / (z_max - z_min)
                mu = min_written - z_min * sigma
        except Exception:
            pass
    elif estimate_mode == "å†å¹´è¿›é¢åˆ†å·®å€¼" and historical_min is not None and historical_max is not None:
        diff = historical_max - historical_min
        if diff > 0:
            try:
                q_bottom = 1.0 - (interview_count / applicants)
                z_bottom = normal_ppf(q_bottom)
                z_top = normal_ppf(1.0 - 1.0 / (applicants + 1.0))
                if abs(z_top - z_bottom) > 1e-6:
                    sigma = diff / (z_top - z_bottom)
                    mu = min_written - z * sigma
                else:
                    sigma = diff / 3.0
                    mu = min_written - z * sigma
            except Exception:
                sigma = diff / 3.0
                mu = min_written - z * sigma
    elif estimate_mode == "æ¯”ä¾‹ä¼°ç®—æœ€é«˜åˆ†" and ratio is not None:
        estimated_max = min_written * ratio
        try:
            z_min = z
            z_max = normal_ppf(1.0 - 1.0 / (applicants + 1.0))
            if abs(z_max - z_min) > 1e-6:
                sigma = (estimated_max - min_written) / (z_max - z_min)
                mu = min_written - z_min * sigma
        except Exception:
            pass

    if mu < 0:
        mu = min_written * 0.5
        sigma = (min_written - mu) / z if z > 0 else sigma
    return mu, sigma


def clamp(value: float, lo: float, hi: float) -> float:
    return max(lo, min(value, hi))


def calc_combined_score(
    written_score: float,
    interview_score: float,
    written_full: float,
    interview_full: float,
    written_weight: float,
) -> float:
    """Return combined score on 0-100 scale."""
    w_ratio = written_weight
    i_ratio = 1 - written_weight
    written_norm = (written_score / written_full) * 100 if written_full else 0
    interview_norm = (interview_score / interview_full) * 100 if interview_full else 0
    return w_ratio * written_norm + i_ratio * interview_norm


def estimate_distribution(
    min_written: float,
    written_full: float,
    interview_mean: float,
    interview_sd: float,
    written_sd: float,
    written_mean_hint: float,
    written_weight: float,
    interview_full: float,
    overall_written_mean: float,
    overall_written_sd: float,
    use_truncated_pool: bool = True,
    written_pool_type: str = "æˆªæ–­æ­£æ€",
    skew_k: float = 0.4,
    # æ–°å¢ï¼šé¢è¯•æˆç»©åˆ†å¸ƒç±»å‹å‚æ•°
    interview_dist_type: str = "æ­£æ€åˆ†å¸ƒ",
) -> Tuple[float, float]:
    """Approximate combined score mean and sd for the interview pool"""
    # ç¬”è¯•åˆ†å¸ƒå¤„ç†ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    if use_truncated_pool:
        mean_written = truncated_normal_mean(overall_written_mean, overall_written_sd, min_written)
        var_written = truncated_normal_var(overall_written_mean, overall_written_sd, min_written)
        sd_written = math.sqrt(max(1e-6, var_written))
        if written_pool_type == "æˆªæ–­åæ€ï¼ˆå³åï¼‰":
            mean_written = mean_written + skew_k * sd_written
            sd_written = sd_written * max(0.3, 1.0 - 0.4 * skew_k)
    else:
        mean_written = overall_written_mean
        sd_written = max(1e-6, overall_written_sd)

    if math.isnan(mean_written) or math.isnan(sd_written):
        mean_written = max(min_written + 0.15 * written_full, written_mean_hint)
        sd_written = max(5.0, written_sd)

    # æ–°å¢ï¼šé¢è¯•æˆç»©åˆ†å¸ƒç±»å‹å¤„ç†
    if interview_dist_type == "å‡åŒ€åˆ†å¸ƒ":
        # å‡åŒ€åˆ†å¸ƒæ–¹å·® = (max-min)Â²/12ï¼Œè¿™é‡Œç®€åŒ–ä¸ºç­‰ä»·æ ‡å‡†å·®
        interview_sd = (interview_full - 0) / math.sqrt(12)
        interview_mean = interview_full / 2  # å‡åŒ€åˆ†å¸ƒå‡å€¼é»˜è®¤ä¸­ç‚¹
    elif interview_dist_type == "åæ€åˆ†å¸ƒï¼ˆå³åï¼‰":
        interview_mean = interview_mean + 0.2 * interview_sd  # å³åè°ƒæ•´å‡å€¼
        interview_sd = interview_sd * 0.8  # å³åè°ƒæ•´æ ‡å‡†å·®
    elif interview_dist_type == "åæ€åˆ†å¸ƒï¼ˆå·¦åï¼‰":
        interview_mean = interview_mean - 0.2 * interview_sd  # å·¦åè°ƒæ•´å‡å€¼
        interview_sd = interview_sd * 0.8  # å·¦åè°ƒæ•´æ ‡å‡†å·®
    # æ­£æ€åˆ†å¸ƒä¿æŒåŸæœ‰é€»è¾‘
    interview_sd = max(5.0, interview_sd)

    # ç»¼åˆåˆ†è®¡ç®—ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    mean_combined = calc_combined_score(
        written_score=mean_written,
        interview_score=interview_mean,
        written_full=written_full,
        interview_full=interview_full,
        written_weight=written_weight,
    )

    w_ratio = written_weight
    i_ratio = 1 - written_weight
    var_combined = (
        (w_ratio ** 2) * ((sd_written / written_full) * 100) ** 2
        + (i_ratio ** 2) * ((interview_sd / interview_full) * 100) ** 2
    )
    sd_combined = math.sqrt(var_combined)
    return mean_combined, sd_combined


def estimate_interview_score(
    interview_mean: float, interview_sd: float, percentile: float, interview_full: float
) -> float:
    percentile = clamp(percentile, 0.0, 100.0)
    p = 1 - percentile / 100
    z = normal_ppf(p)
    estimated = interview_mean + z * interview_sd
    return clamp(estimated, 0, interview_full)


def compute_probability(
    entered_interview: bool,
    written_score: float,
    interview_score_est: float,
    written_full: float,
    interview_full: float,
    written_weight: float,
    min_written: float,
    interview_mean: float,
    interview_sd: float,
    written_sd: float,
    written_mean_hint: float,
    admit_count: float,
    interview_count: float,
    overall_written_mean: float,
    overall_written_sd: float,
    use_truncated_pool: bool = True,
    written_pool_type: str = "æˆªæ–­æ­£æ€",
    skew_k: float = 0.4,
    use_mc: bool = False,
    mc_samples: int = 0,
    applicants: int = 0,
    # æ–°å¢å‚æ•°
    compute_mode: str = "è§£æè®¡ç®—",  # è®¡ç®—æ–¹å¼ï¼šè§£æè®¡ç®—/è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
    interview_dist_type: str = "æ­£æ€åˆ†å¸ƒ",  # é¢è¯•æˆç»©åˆ†å¸ƒç±»å‹
) -> Tuple[float, float, float, float, float]:
    """Return probability, user_combined, cutoff, sd_combined, mean_combined."""
    # è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿè·¯å¾„ï¼ˆä»…é«˜çº§æ¨¡å¼å¯é€‰ï¼‰
    if use_mc and compute_mode == "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ":
        N = int(applicants)
        M = int(interview_count)
        K = int(admit_count)
        mu = overall_written_mean
        sigma = overall_written_sd

        success = 0
        cutoffs = []
        combined_sds = []
        mean_combined_list = []

        for _ in range(int(mc_samples)):
            others = np.random.normal(loc=mu, scale=sigma, size=N - 1)
            all_written = np.concatenate([others, np.array([written_score])])
            top_idx = np.argsort(all_written)[-M:]
            user_index = len(all_written) - 1
            user_in_top = user_index in top_idx
            top_written = all_written[top_idx]
            
            # å¤è¯•åˆ†å¸ƒè®¡ç®—æ¨¡å¼ï¼šåº”ç”¨ç¬”è¯•åˆ†å¸ƒç±»å‹è°ƒæ•´
            if written_pool_type == "æˆªæ–­åæ€ï¼ˆå³åï¼‰":
                top_written = top_written + skew_k * np.std(top_written)
            elif written_pool_type == "åæ€ï¼ˆå·¦åï¼‰":
                top_written = top_written - skew_k * np.std(top_written)

            # é¢è¯•æˆç»©åˆ†å¸ƒç±»å‹é‡‡æ ·è°ƒæ•´
            if interview_dist_type == "å‡åŒ€åˆ†å¸ƒ":
                top_interview = np.random.uniform(0, interview_full, size=M)
            elif interview_dist_type == "åæ€åˆ†å¸ƒï¼ˆå³åï¼‰":
                # å¯¹æ•°æ­£æ€åˆ†å¸ƒæ¨¡æ‹Ÿå³å
                top_interview = np.random.lognormal(
                    mean=np.log(interview_mean), 
                    sigma=interview_sd/interview_mean, 
                    size=M
                )
            elif interview_dist_type == "åæ€åˆ†å¸ƒï¼ˆå·¦åï¼‰":
                # åè½¬å¯¹æ•°æ­£æ€åˆ†å¸ƒæ¨¡æ‹Ÿå·¦å
                top_interview = interview_full - np.random.lognormal(
                    mean=np.log(interview_mean), 
                    sigma=interview_sd/interview_mean, 
                    size=M
                )
            else:  # æ­£æ€åˆ†å¸ƒ
                top_interview = np.random.normal(loc=interview_mean, scale=interview_sd, size=M)

            # ä¿®æ­£é¢è¯•åˆ†æ•°èŒƒå›´
            top_interview = np.clip(top_interview, 0, interview_full)

            if user_in_top:
                pos = list(top_idx).index(user_index)
                top_interview[pos] = interview_score_est

            # è®¡ç®—ç»¼åˆåˆ†
            written_norm = (top_written / written_full) * 100
            interview_norm = (top_interview / interview_full) * 100
            combined = written_weight * written_norm + (1 - written_weight) * interview_norm

            cutoff_sim = np.sort(combined)[-K] if K <= M else np.min(combined)
            cutoffs.append(float(cutoff_sim))
            mean_combined_list.append(float(np.mean(combined)))
            combined_sds.append(float(np.std(combined, ddof=1)))

            if user_in_top:
                user_combined = calc_combined_score(
                    written_score=written_score,
                    interview_score=interview_score_est,
                    written_full=written_full,
                    interview_full=interview_full,
                    written_weight=written_weight,
                )
                if user_combined >= cutoff_sim:
                    success += 1

        prob = success / int(mc_samples) if mc_samples > 0 else 0.0
        user_combined = calc_combined_score(
            written_score=written_score,
            interview_score=interview_score_est,
            written_full=written_full,
            interview_full=interview_full,
            written_weight=written_weight,
        )
        cutoff = float(np.mean(cutoffs)) if len(cutoffs) else 0.0
        sd_combined = float(np.mean(combined_sds)) if len(combined_sds) else 0.0
        mean_combined = float(np.mean(mean_combined_list)) if len(mean_combined_list) else 0.0
        return prob, user_combined, cutoff, sd_combined, mean_combined

    # è§£æè®¡ç®—è·¯å¾„ï¼ˆé»˜è®¤ï¼‰
    mean_combined, sd_combined = estimate_distribution(
        min_written=min_written,
        written_full=written_full,
        interview_mean=interview_mean,
        interview_sd=interview_sd,
        written_sd=written_sd,
        written_mean_hint=written_mean_hint,
        written_weight=written_weight,
        interview_full=interview_full,
        overall_written_mean=overall_written_mean,
        overall_written_sd=overall_written_sd,
        use_truncated_pool=use_truncated_pool,
        written_pool_type=written_pool_type,
        skew_k=skew_k,
        interview_dist_type=interview_dist_type,  # ä¼ é€’é¢è¯•åˆ†å¸ƒç±»å‹
    )

    user_combined = calc_combined_score(
        written_score=written_score,
        interview_score=interview_score_est,
        written_full=written_full,
        interview_full=interview_full,
        written_weight=written_weight,
    )

    if interview_count <= 0 or admit_count <= 0:
        return 0.0, user_combined, 0.0, sd_combined, mean_combined

    admit_ratio = clamp(admit_count / interview_count, 0.0, 1.0)
    if admit_ratio >= 1:
        return 1.0, user_combined, 0.0, sd_combined, mean_combined

    cutoff_quantile = 1 - admit_ratio
    z_line = normal_ppf(cutoff_quantile)
    cutoff_score = mean_combined + z_line * sd_combined

    if sd_combined <= 1e-6:
        probability = 1.0 if user_combined >= cutoff_score else 0.0
    else:
        probability = 1 - normal_cdf((cutoff_score - user_combined) / sd_combined)

    probability = clamp(probability, 0.0, 1.0)
    return probability, user_combined, cutoff_score, sd_combined, mean_combined


def show_fireworks():
    """Render a simple fireworks-like animation."""
    fireworks_html = """
    <div class="fireworks">
      <div class="after"></div>
      <div class="before"></div>
    </div>
    <style>
      .fireworks, .fireworks::before, .fireworks::after {
        position: fixed;
        top: 50%;
        left: 50%;
        width: 8px;
        height: 8px;
        background: transparent;
        pointer-events: none;
        transform: translate(-50%, -50%);
        box-shadow: -60px -60px #ff4d4f, 0 -70px #ffc53d, 60px -60px #40a9ff,
                    -70px 0 #73d13d, 70px 0 #9254de, -60px 60px #ff85c0,
                    0 70px #5cdbd3, 60px 60px #ffec3d;
        animation: pop 900ms ease-out forwards;
        opacity: 0.9;
      }
      .fireworks::before, .fireworks::after {
        content: "";
        display: block;
      }
      .fireworks::before {
        box-shadow: -50px -80px #ff4d4f, 50px -80px #ffc53d,
                    -80px 50px #40a9ff, 80px 50px #73d13d,
                    -80px -20px #9254de, 80px -20px #ff85c0,
                    -20px 80px #5cdbd3, 20px 80px #ffec3d;
        animation: pop 1000ms ease-out forwards;
      }
      .fireworks::after {
        box-shadow: -30px -90px #73d13d, 30px -90px #9254de,
                    -90px 30px #ff4d4f, 90px 30px #5cdbd3,
                    -90px -10px #ffc53d, 90px -10px #40a9ff,
                    -10px 90px #ff85c0, 10px 90px #ffec3d;
        animation: pop 1100ms ease-out forwards;
      }
      @keyframes pop {
        0% { transform: translate(-50%, -50%) scale(0.2); opacity: 1; }
        80% { opacity: 1; }
        100% { transform: translate(-50%, -50%) scale(1.1); opacity: 0; }
      }
    </style>
    """
    st.components.v1.html(fireworks_html, height=0, width=0)


def render_feedback(probability: float, mode: str, entered: bool):
    if not entered:
        st.write("âŒ æœªè¿›å…¥é¢è¯•ï¼Œæ— æ³•è¯„ä¼°å½•å–æ¦‚ç‡ã€‚")
        return

    if mode.startswith("å®¢è§‚"):
        tiers = [
            (0.8, "ğŸ‰ å¾ˆæœ‰å¸Œæœ›ï¼ä¸Šå²¸åœ¨æœ›ï¼"),
            (0.6, "ä¸é”™çš„æœºä¼šï¼Œä¿æŒä¿¡å¿ƒ"),
            (0.4, "æ½œåŠ›å¾ˆå¤§ï¼Œç»§ç»­åŠªåŠ›"),
            (0.2, "åŠ æ²¹ï¼Œè¿˜æœ‰æˆ"),
            (0.0, "å®¢è§‚è¯„ä¼°ï¼šè¿›é¢å³æœ‰æœºä¼š âœ¨"),
        ]
    else:
        tiers = [
            (0.9, "ğŸ‰ ç¥è´ºï¼å¤§æ¦‚ç‡ä¸Šå²¸ï¼Œæ”¾çƒŸèŠ±åº†ç¥ï¼"),
            (0.8, "å¾ˆç¨³ï¼Œä¿æŒèŠ‚å¥å³å¯"),
            (0.6, "æœ‰æˆï¼Œè®¤çœŸå‡†å¤‡é¢è¯•ç»†èŠ‚"),
            (0.4, "äº”äº”å¼€ï¼Œè¡¥é½çŸ­æ¿æå‡ç¨³å®šæ€§"),
            (0.2, "éœ€è¦åŠ æ²¹ï¼Œé’ˆå¯¹å¼±é¡¹å†²åˆº"),
            (0.0, "é£é™©è¾ƒé«˜ï¼Œå°½é‡å¤šåšå¤‡é€‰"),
        ]

    for threshold, text in tiers:
        if probability >= threshold:
            st.write(text)
            break

    if probability >= 0.9:
        show_fireworks()
        st.balloons()


def render_distribution_chart(mean_c: float, sd_c: float, user_c: float, cutoff_c: float):
    if sd_c <= 1e-6:
        st.info("åˆ†å¸ƒæ ‡å‡†å·®è¿‡å°ï¼Œæ— æ³•ç»˜åˆ¶æ›²çº¿ã€‚")
        return

    x_min = mean_c - 3.5 * sd_c
    x_max = mean_c + 3.5 * sd_c
    xs = np.linspace(x_min, x_max, 201)  # ä¼˜åŒ–ï¼šæ›¿æ¢åˆ—è¡¨æ¨å¯¼å¼
    ys = [normal_pdf((x - mean_c) / sd_c) / sd_c for x in xs]
    df = pd.DataFrame({"score": xs, "density": ys})

    base = alt.Chart(df).mark_line(color="#1890ff", strokeWidth=2).encode(
        x=alt.X("score", title="ç»¼åˆåˆ†"),
        y=alt.Y("density", title="å¯†åº¦", axis=alt.Axis(labels=False)),
    )

    user_rule = (
        alt.Chart(pd.DataFrame({"score": [user_c], "label": ["ä½ "]}))
        .mark_rule(color="#fa541c", strokeWidth=2, strokeDash=[5, 3])
        .encode(x="score")
    )

    cutoff_rule = (
        alt.Chart(pd.DataFrame({"score": [cutoff_c], "label": ["å½•å–çº¿"]}))
        .mark_rule(color="#52c41a", strokeWidth=2)
        .encode(x="score")
    )

    text_layer = (
        alt.Chart(
            pd.DataFrame(
                {
                    "score": [user_c, cutoff_c],
                    "density": [max(ys) * 0.9, max(ys) * 0.8],
                    "label": ["ä½ çš„åˆ†", "é¢„è®¡å½•å–çº¿"],
                }
            )
        )
        .mark_text(dy=-6, fontSize=11)
        .encode(x="score", y="density", text="label", color=alt.value("#595959"))
    )

    chart = base + user_rule + cutoff_rule + text_layer
    st.altair_chart(chart, use_container_width=True)


# ===================== æ–°å¢ï¼šå‹æƒ…è¯„ä¼°ä¸»ç•Œé¢ï¼ˆåˆçº§/é«˜çº§æ¨¡å¼ï¼‰ =====================
def main():
    st.set_page_config(page_title="ä¸Šå²¸æ¦‚ç‡ä¼°ç®—å™¨", page_icon="ğŸ“", layout="wide")
    st.title("ğŸ“ ä¸Šå²¸æ¦‚ç‡ä¼°ç®—å™¨ - å‹æƒ…è¯„ä¼°ç‰ˆ")

    # 1. æ¨¡å¼é€‰æ‹©ï¼šåˆçº§/é«˜çº§
    mode_level = st.sidebar.radio(
        "é€‰æ‹©è¯„ä¼°æ¨¡å¼",
        ["åˆçº§æ¨¡å¼", "é«˜çº§æ¨¡å¼"],
        help="åˆçº§æ¨¡å¼ï¼šä»…ä¿ç•™æ ¸å¿ƒç¬”è¯•åˆ†å¸ƒä¼°ç®—ï¼›é«˜çº§æ¨¡å¼ï¼šå…¨åŠŸèƒ½æ‰©å±•"
    )

    # 2. é€šç”¨åŸºç¡€å‚æ•°ï¼ˆæ‰€æœ‰æ¨¡å¼å…±äº«ï¼‰
    st.sidebar.header("ğŸ“ åŸºç¡€å‚æ•°")
    applicants = st.sidebar.number_input("æŠ¥åæ€»äººæ•°", min_value=1, value=1000)
    interview_count = st.sidebar.number_input("è¿›é¢äººæ•°", min_value=1, value=200)
    admit_count = st.sidebar.number_input("å½•å–äººæ•°", min_value=1, value=50)
    min_written = st.sidebar.number_input("è¿›é¢æœ€ä½ç¬”è¯•åˆ†", min_value=0.0, value=120.0)
    written_full = st.sidebar.number_input("ç¬”è¯•æ»¡åˆ†", min_value=1.0, value=200.0)
    interview_full = st.sidebar.number_input("é¢è¯•æ»¡åˆ†", min_value=1.0, value=100.0)
    written_weight_pct = st.sidebar.slider("ç¬”è¯•æƒé‡(%)", min_value=0, max_value=100, value=60)
    written_weight = written_weight_pct / 100

    # 3. åˆçº§æ¨¡å¼ï¼šä»…ä¿ç•™ç¬”è¯•åˆ†å¸ƒä¼°ç®—
    if mode_level == "åˆçº§æ¨¡å¼":
        st.header("ğŸ”° åˆçº§æ¨¡å¼ - ç¬”è¯•åˆ†å¸ƒä¼°ç®—")
        
        # ä»…æ˜¾ç¤ºç¬”è¯•åˆ†å¸ƒä¼°ç®—æŒ‰é’®
        if st.button("ğŸ“Š å¼€å§‹ç¬”è¯•åˆ†å¸ƒä¼°ç®—"):
            # ä¼°ç®—æ•´ä½“ç¬”è¯•å‡å€¼å’Œæ ‡å‡†å·®
            overall_written_mean, overall_written_sd = estimate_overall_from_data(
                applicants=applicants,
                interview_count=interview_count,
                min_written=min_written,
                estimate_mode="å·²çŸ¥æœ€é«˜åˆ†",
                known_max=written_full
            )
            st.subheader("ğŸ“ˆ ç¬”è¯•åˆ†å¸ƒä¼°ç®—ç»“æœ")
            st.write(f"æ•´ä½“ç¬”è¯•å¹³å‡åˆ†ï¼š{overall_written_mean:.2f}")
            st.write(f"æ•´ä½“ç¬”è¯•æ ‡å‡†å·®ï¼š{overall_written_sd:.2f}")
            st.write(f"è¿›é¢ç¬”è¯•åˆ†ä¸‹é™ï¼š{min_written:.2f}")

            # ç»˜åˆ¶ç¬”è¯•åˆ†å¸ƒæ›²çº¿
            x_min = overall_written_mean - 3.5 * overall_written_sd
            x_max = overall_written_mean + 3.5 * overall_written_sd
            xs = np.linspace(x_min, x_max, 201)
            ys = [normal_pdf((x - overall_written_mean) / overall_written_sd) / overall_written_sd for x in xs]
            df = pd.DataFrame({"ç¬”è¯•åˆ†æ•°": xs, "å¯†åº¦": ys})
            
            # æ ‡è®°è¿›é¢çº¿
            cutoff_line = alt.Chart(pd.DataFrame({"x": [min_written]})).mark_rule(color="#ff4d4f", strokeWidth=2).encode(x="x")
            base = alt.Chart(df).mark_line(color="#1890ff").encode(
                x=alt.X("ç¬”è¯•åˆ†æ•°", title="ç¬”è¯•åˆ†æ•°"),
                y=alt.Y("å¯†åº¦", title="å¯†åº¦")
            )
            st.altair_chart(base + cutoff_line, use_container_width=True)

    # 4. é«˜çº§æ¨¡å¼ï¼šå…¨åŠŸèƒ½æ‰©å±•
    elif mode_level == "é«˜çº§æ¨¡å¼":
        st.header("âš¡ é«˜çº§æ¨¡å¼ - å…¨åŠŸèƒ½è¯„ä¼°")
        
        # 4.1 è®¡ç®—æ–¹å¼é€‰æ‹©
        compute_mode = st.sidebar.selectbox(
            "è®¡ç®—æ–¹å¼",
            ["è§£æè®¡ç®—", "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ"],
            help="è§£æè®¡ç®—ï¼šå¿«é€Ÿä¼°ç®—ï¼›è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼šæ›´ç²¾å‡†ä½†è€—æ—¶"
        )
        mc_samples = st.sidebar.number_input(
            "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ¬¡æ•°",
            min_value=1000,
            max_value=100000,
            value=50000,
            step=1000,
            disabled=(compute_mode != "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ")
        )

        # 4.2 å¤è¯•åˆ†å¸ƒè®¡ç®—æ¨¡å¼
        st.sidebar.header("ğŸ” å¤è¯•åˆ†å¸ƒå‚æ•°")
        distribution_mode = st.sidebar.radio(
            "å¤è¯•åˆ†å¸ƒè®¡ç®—æ¨¡å¼",
            ["åŸºç¡€æ¨¡å¼", "è¿›é˜¶æ¨¡å¼ï¼ˆå«åæ€è°ƒæ•´ï¼‰"],
            help="è¿›é˜¶æ¨¡å¼æ”¯æŒç¬”è¯•åˆ†å¸ƒç±»å‹è°ƒæ•´"
        )
        
        # 4.3 ç¬”è¯•åˆ†å¸ƒç±»å‹ï¼ˆè¿›é˜¶æ¨¡å¼æ˜¾ç¤ºï¼‰
        written_pool_type = st.sidebar.selectbox(
            "ç¬”è¯•åˆ†å¸ƒç±»å‹",
            ["æˆªæ–­æ­£æ€", "æˆªæ–­åæ€ï¼ˆå³åï¼‰", "åæ€ï¼ˆå·¦åï¼‰"],
            disabled=(distribution_mode != "è¿›é˜¶æ¨¡å¼ï¼ˆå«åæ€è°ƒæ•´ï¼‰")
        )
        skew_k = st.sidebar.slider(
            "åæ€å¼ºåº¦",
            min_value=0.0,
            max_value=1.0,
            value=0.4,
            disabled=(distribution_mode != "è¿›é˜¶æ¨¡å¼ï¼ˆå«åæ€è°ƒæ•´ï¼‰")
        )

        # 4.4 é¢è¯•æˆç»©åˆ†å¸ƒç±»å‹
        interview_dist_type = st.sidebar.selectbox(
            "é¢è¯•æˆç»©åˆ†å¸ƒç±»å‹",
            ["æ­£æ€åˆ†å¸ƒ", "å‡åŒ€åˆ†å¸ƒ", "åæ€åˆ†å¸ƒï¼ˆå³åï¼‰", "åæ€åˆ†å¸ƒï¼ˆå·¦åï¼‰"],
            help="é€‰æ‹©é¢è¯•æˆç»©çš„åˆ†å¸ƒç‰¹å¾"
        )
        interview_mean = st.sidebar.number_input(
            "é¢è¯•å¹³å‡åˆ†",
            min_value=0.0,
            max_value=interview_full,
            value=80.0
        )
        interview_sd = st.sidebar.number_input(
            "é¢è¯•æ ‡å‡†å·®",
            min_value=0.1,
            max_value=20.0,
            value=5.0
        )

        # 4.5 ä¸ªäººæˆç»©è¾“å…¥
        st.sidebar.header("ğŸ‘¤ ä¸ªäººæˆç»©")
        written_score = st.sidebar.number_input("ä½ çš„ç¬”è¯•åˆ†æ•°", min_value=0.0, max_value=written_full, value=130.0)
        interview_percentile = st.sidebar.slider(
            "ä½ çš„é¢è¯•æˆç»©ç™¾åˆ†ä½ï¼ˆ0=æœ€å¥½ï¼Œ100=æœ€å·®ï¼‰",
            min_value=0.0,
            max_value=100.0,
            value=20.0
        )
        interview_score_est = estimate_interview_score(
            interview_mean=interview_mean,
            interview_sd=interview_sd,
            percentile=interview_percentile,
            interview_full=interview_full
        )

        # 4.6 æ ¸å¿ƒè®¡ç®—æŒ‰é’®
        if st.button("ğŸš€ å¼€å§‹ç»¼åˆå½•å–æ¦‚ç‡è¯„ä¼°"):
            # ä¼°ç®—æ•´ä½“ç¬”è¯•åˆ†å¸ƒ
            overall_written_mean, overall_written_sd = estimate_overall_from_data(
                applicants=applicants,
                interview_count=interview_count,
                min_written=min_written,
                estimate_mode="å·²çŸ¥æœ€é«˜åˆ†",
                known_max=written_full
            )

            # è®¡ç®—å½•å–æ¦‚ç‡
            probability, user_combined, cutoff_score, sd_combined, mean_combined = compute_probability(
                entered_interview=True,
                written_score=written_score,
                interview_score_est=interview_score_est,
                written_full=written_full,
                interview_full=interview_full,
                written_weight=written_weight,
                min_written=min_written,
                interview_mean=interview_mean,
                interview_sd=interview_sd,
                written_sd=overall_written_sd,
                written_mean_hint=overall_written_mean,
                admit_count=admit_count,
                interview_count=interview_count,
                overall_written_mean=overall_written_mean,
                overall_written_sd=overall_written_sd,
                use_truncated_pool=(distribution_mode == "è¿›é˜¶æ¨¡å¼ï¼ˆå«åæ€è°ƒæ•´ï¼‰"),
                written_pool_type=written_pool_type,
                skew_k=skew_k,
                use_mc=(compute_mode == "è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ"),
                mc_samples=mc_samples,
                applicants=applicants,
                compute_mode=compute_mode,
                interview_dist_type=interview_dist_type
            )

            # å±•ç¤ºç»“æœ
            st.subheader("ğŸ¯ è¯„ä¼°ç»“æœ")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ä½ çš„ç»¼åˆåˆ†", f"{user_combined:.2f}")
            with col2:
                st.metric("é¢„è®¡å½•å–çº¿", f"{cutoff_score:.2f}")
            with col3:
                st.metric("å½•å–æ¦‚ç‡", f"{probability:.2%}")

            # åé¦ˆæç¤º
            render_feedback(probability, "é«˜çº§æ¨¡å¼", entered=True)

            # ç»˜åˆ¶ç»¼åˆåˆ†åˆ†å¸ƒ
            st.subheader("ğŸ“Š ç»¼åˆåˆ†åˆ†å¸ƒ")
            render_distribution_chart(mean_combined, sd_combined, user_combined, cutoff_score)

            # å±•ç¤ºè¯¦ç»†å‚æ•°
            with st.expander("ğŸ“‹ è¯¦ç»†å‚æ•°æ˜ç»†"):
                st.write(f"è®¡ç®—æ–¹å¼ï¼š{compute_mode}")
                st.write(f"å¤è¯•åˆ†å¸ƒæ¨¡å¼ï¼š{distribution_mode}")
                st.write(f"ç¬”è¯•åˆ†å¸ƒç±»å‹ï¼š{written_pool_type}")
                st.write(f"é¢è¯•åˆ†å¸ƒç±»å‹ï¼š{interview_dist_type}")
                st.write(f"æ•´ä½“ç¬”è¯•å¹³å‡åˆ†ï¼š{overall_written_mean:.2f}")
                st.write(f"æ•´ä½“ç¬”è¯•æ ‡å‡†å·®ï¼š{overall_written_sd:.2f}")
                st.write(f"é¢è¯•å¹³å‡åˆ†ï¼š{interview_mean:.2f}")
                st.write(f"é¢è¯•æ ‡å‡†å·®ï¼š{interview_sd:.2f}")

# ====================== Streamlit ä¸»ç¨‹åºå…¥å£ï¼ˆç²˜è´´åˆ°æ–‡ä»¶æœ«å°¾ï¼‰ ======================
def main():
    # é¡µé¢åŸºç¡€é…ç½®
    st.set_page_config(
        page_title="å…¬è€ƒå½•å–æ¦‚ç‡ä¼°ç®—",
        page_icon="ğŸ“Š",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # é¡µé¢æ ‡é¢˜
    st.title("ğŸ“Š é¢è¯•/å½•å–æ¦‚ç‡ä¼°ç®—å·¥å…·")
    st.divider()

    # ä¾§è¾¹æ ï¼šå‚æ•°è¾“å…¥åŒº
    with st.sidebar:
        st.header("âš™ï¸ åŸºç¡€å‚æ•°è®¾ç½®")
        applicants = st.number_input("æ€»æŠ¥åäººæ•°", min_value=1, value=1000, step=10)
        interview_count = st.number_input("è¿›é¢äººæ•°", min_value=1, value=200, step=10)
        min_written = st.number_input("è¿›é¢æœ€ä½ç¬”è¯•åˆ†", min_value=0.0, value=120.0, step=1.0)
        written_full = st.number_input("ç¬”è¯•æ»¡åˆ†", min_value=1.0, value=200.0, step=1.0)
        interview_full = st.number_input("é¢è¯•æ»¡åˆ†", min_value=1.0, value=100.0, step=1.0)
        written_weight = st.slider("ç¬”è¯•æƒé‡ï¼ˆ0-1ï¼‰", 0.0, 1.0, 0.6, step=0.05)
        admit_count = st.number_input("æœ€ç»ˆå½•å–äººæ•°", min_value=1, value=50, step=5)
        interview_mean = st.number_input("é¢è¯•å¹³å‡åˆ†ï¼ˆé¢„ä¼°ï¼‰", min_value=0.0, value=85.0, step=0.5)
        interview_sd = st.number_input("é¢è¯•åˆ†æ ‡å‡†å·®ï¼ˆé¢„ä¼°ï¼‰", min_value=0.1, value=5.0, step=0.1)
        user_written = st.number_input("ä½ çš„ç¬”è¯•åˆ†æ•°", min_value=0.0, value=130.0, step=0.5)
        user_interview_est = st.number_input("ä½ çš„é¢è¯•é¢„ä¼°åˆ†", min_value=0.0, value=88.0, step=0.5)

        st.divider()
        st.header("ğŸ”§ é«˜çº§è®¾ç½®")
        estimate_mode = st.selectbox("ç¬”è¯•åˆ†ä¼°ç®—æ¨¡å¼", ["å·²çŸ¥æœ€é«˜åˆ†", "å†å¹´è¿›é¢åˆ†å·®å€¼", "æ¯”ä¾‹ä¼°ç®—æœ€é«˜åˆ†"])
        known_max = st.number_input("å·²çŸ¥ç¬”è¯•æœ€é«˜åˆ†", min_value=min_written, value=180.0, step=1.0)
        historical_min = st.number_input("å†å¹´è¿›é¢æœ€ä½åˆ†ï¼ˆä»…å·®å€¼æ¨¡å¼ï¼‰", min_value=0.0, value=110.0, step=1.0)
        historical_max = st.number_input("å†å¹´è¿›é¢æœ€é«˜åˆ†ï¼ˆä»…å·®å€¼æ¨¡å¼ï¼‰", min_value=historical_min, value=170.0, step=1.0)
        ratio = st.number_input("æœ€é«˜åˆ†/è¿›é¢æœ€ä½åˆ†ï¼ˆä»…æ¯”ä¾‹æ¨¡å¼ï¼‰", min_value=1.0, value=1.5, step=0.1)
        use_truncated_pool = st.checkbox("ä½¿ç”¨æˆªæ–­ç¬”è¯•åˆ†å¸ƒï¼ˆæ›´ç²¾å‡†ï¼‰", value=True)
        written_pool_type = st.selectbox("ç¬”è¯•åˆ†å¸ƒç±»å‹", ["æˆªæ–­æ­£æ€", "æˆªæ–­åæ€ï¼ˆå³åï¼‰"])
        skew_k = st.slider("åæ€ç³»æ•°ï¼ˆä»…åæ€æ¨¡å¼ï¼‰", 0.0, 1.0, 0.4, step=0.1)
        use_mc = st.checkbox("å¯ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼ˆæ›´ç²¾å‡†ä½†æ…¢ï¼‰", value=False)
        mc_samples = st.number_input("æ¨¡æ‹Ÿæ¬¡æ•°ï¼ˆä»…æ¨¡æ‹Ÿæ¨¡å¼ï¼‰", min_value=100, max_value=10000, value=1000, step=100)

    # 1. ä¼°ç®—æ•´ä½“ç¬”è¯•å‡å€¼å’Œæ ‡å‡†å·®
    overall_written_mean, overall_written_sd = estimate_overall_from_data(
        applicants=applicants,
        interview_count=interview_count,
        min_written=min_written,
        estimate_mode=estimate_mode,
        known_max=known_max,
        historical_min=historical_min,
        historical_max=historical_max,
        ratio=ratio,
    )

    # 2. åˆ¤æ–­æ˜¯å¦è¿›é¢
    entered_interview = user_written >= min_written

    # 3. è®¡ç®—å½•å–æ¦‚ç‡
    prob, user_combined, cutoff, sd_combined, mean_combined = compute_probability(
        entered_interview=entered_interview,
        written_score=user_written,
        interview_score_est=user_interview_est,
        written_full=written_full,
        interview_full=interview_full,
        written_weight=written_weight,
        min_written=min_written,
        interview_mean=interview_mean,
        interview_sd=interview_sd,
        written_sd=overall_written_sd,
        written_mean_hint=overall_written_mean,
        admit_count=admit_count,
        interview_count=interview_count,
        overall_written_mean=overall_written_mean,
        overall_written_sd=overall_written_sd,
        use_truncated_pool=use_truncated_pool,
        written_pool_type=written_pool_type,
        skew_k=skew_k,
        use_mc=use_mc,
        mc_samples=mc_samples,
        applicants=applicants,
    )

    # 4. æ¸²æŸ“ç»“æœåŒºåŸŸ
    st.subheader("ğŸ“ˆ ä¼°ç®—ç»“æœ")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("æ˜¯å¦è¿›é¢", "âœ… æ˜¯" if entered_interview else "âŒ å¦")
    with col2:
        st.metric("ä½ çš„ç»¼åˆåˆ†", f"{user_combined:.2f}")
    with col3:
        st.metric("é¢„ä¼°å½•å–çº¿", f"{cutoff:.2f}")
    with col4:
        st.metric("å½•å–æ¦‚ç‡", f"{prob:.2%}")

    # 5. æ¸²æŸ“åé¦ˆæç¤º
    render_feedback(probability=prob, mode="ä¸»è§‚", entered=entered_interview)

    # 6. ç»˜åˆ¶ç»¼åˆåˆ†åˆ†å¸ƒå›¾è¡¨
    st.subheader("ğŸ“Š ç»¼åˆåˆ†åˆ†å¸ƒæ›²çº¿")
    render_distribution_chart(mean_c=mean_combined, sd_c=sd_combined, user_c=user_combined, cutoff_c=cutoff)

    # 7. æ˜¾ç¤ºé«˜çº§ä¿¡æ¯ï¼ˆæŠ˜å é¢æ¿ï¼‰
    with st.expander("ğŸ” è¯¦ç»†å‚æ•°ä¸è®¡ç®—è¿‡ç¨‹ï¼ˆé«˜çº§ï¼‰", expanded=False):
        st.write("### æ•´ä½“ç¬”è¯•åˆ†æ•°ä¼°ç®—")
        st.write(f"- æ•´ä½“ç¬”è¯•å‡å€¼ï¼š{overall_written_mean:.2f}")
        st.write(f"- æ•´ä½“ç¬”è¯•æ ‡å‡†å·®ï¼š{overall_written_sd:.2f}")
        st.write("### ç»¼åˆåˆ†åˆ†å¸ƒå‚æ•°")
        st.write(f"- è¿›é¢äººç¾¤ç»¼åˆåˆ†å‡å€¼ï¼š{mean_combined:.2f}")
        st.write(f"- è¿›é¢äººç¾¤ç»¼åˆåˆ†æ ‡å‡†å·®ï¼š{sd_combined:.2f}")

# å¯åŠ¨ä¸»ç¨‹åº
if __name__ == "__main__":
    main()
